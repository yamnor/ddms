---
title: 📘 実習⑤ | 機械学習モデルを構築する
---

前章までで、第一原理計算から体積（Volume）に対するエネルギー（Energy）のデータを取得し、CSVファイルとして保存しました。本章では、このデータを使って**機械学習モデルを構築**し、データ駆動型アプローチの基礎を体験します。

## 機械学習による Volume-Energy 相関の予測

前章で取得したVolume-Energy データは、物理法則に基づいて計算された「教師データ」です。このデータから、**体積からエネルギーを予測するモデル**を構築してみましょう。

このモデルが完成すれば、高コストな第一原理計算を実行せずに、シリコン結晶について、ある体積の値に対するエネルギーをすぐに推定できるようになります。

### データの準備

まず、前章で保存した CSV 形式のファイルを読み込み、機械学習用にデータを準備します。

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# 前章で作成したDataFrameを使用します
# （第5章のコードを実行済みであることを前提とします）
df = pd.read_csv('si_eos_result.csv')

# 特徴量（入力）とターゲット（出力）を定義
X = df[['Volume']].values  # 入力: 体積
y = df['Energy'].values     # 出力: エネルギー

print(f"データ数: {len(X)}")
print(f"Volume範囲: {X.min():.3f} ～ {X.max():.3f} Å³")
print(f"Energy範囲: {y.min():.4f} ～ {y.max():.4f} eV")
```

**出力例：**

```term
データ数: 11
Volume範囲: 37.672 ～ 42.476 Å³
Energy範囲: -10.7955 ～ -10.7064 eV
```

### データの分割と前処理

機械学習では、モデルの性能を評価するために、データを「学習用」と「検証用」に分割します。これは、未知のデータに対する予測性能（汎化性能）を疑似的に評価するための標準的な手順です。

```python
# データを学習用（80%）と検証用（20%）に分割
# random_stateを固定することで、再現性を確保します
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# データの標準化（オプション：スケールを揃える）
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()

print(f"学習データ数: {len(X_train)}")
print(f"検証データ数: {len(X_test)}")
```

**出力例：**

```term
学習データ数: 8
検証データ数: 3
```

### モデルの学習：多項式回帰

Volume-Energy の関係は非線形（放物線状）なので、**多項式回帰**を使用します。これは、線形回帰を拡張して、特徴量の高次項を追加することで非線形関係を表現する手法です。Volume-Energy 曲線は谷底付近で放物線的に振る舞うため、2次多項式は物理的にも妥当な近似になっています。

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# 多項式回帰パイプラインの構築
# degree=2: 2次多項式（放物線）を仮定
poly_reg = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])

# モデルの学習
poly_reg.fit(X_train_scaled, y_train_scaled)

# 予測
y_train_pred_scaled = poly_reg.predict(X_train_scaled)
y_test_pred_scaled = poly_reg.predict(X_test_scaled)

# 標準化を元に戻す
y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).ravel()
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()

# 性能評価
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print(f"--- モデル性能 ---")
print(f"学習データ R²: {train_r2:.6f}")
print(f"検証データ R²: {test_r2:.6f}")
print(f"学習データ RMSE: {train_rmse:.6f} eV")
print(f"検証データ RMSE: {test_rmse:.6f} eV")
```

**出力例：**

```term
--- モデル性能 ---
学習データ R²: 0.999010
検証データ R²: 0.997646
学習データ RMSE: 0.000688 eV
検証データ RMSE: 0.001889 eV
```

**R²（決定係数）** は1に近いほど良い予測性能を示します。0.999以上であれば、ほぼ完璧な予測ができていることを意味します。

ただし、今回はデータ点数が 11 点と少なく、ほとんど補間問題に近い設定であるため、「大規模・高次元データでも常にこの精度が出る」という意味ではない点に注意してください。
{.warn}

### 予測結果の可視化

学習したモデルが、実際のデータをどれだけ正確に再現できているか、グラフで確認しましょう。

```python
# 予測用の連続的なVolume値を作成
volume_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
volume_range_scaled = scaler_X.transform(volume_range)
energy_pred_scaled = poly_reg.predict(volume_range_scaled)
energy_pred = scaler_y.inverse_transform(energy_pred_scaled.reshape(-1, 1)).ravel()

# グラフの描画
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, color='blue', s=100, label='Training Data', zorder=3)
plt.scatter(X_test, y_test, color='red', s=100, marker='^', label='Test Data', zorder=3)
plt.plot(volume_range, energy_pred, 'g-', linewidth=2, label='ML Prediction', alpha=0.7)
plt.xlabel('Volume ($\AA^3$)')
plt.ylabel('Energy (eV)')
plt.title('Machine Learning Prediction: Volume → Energy')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**出力例：**

![](https://gyazo.com/b590f1254a96fde6ab52d39ccf1b662d.png)

青い点が学習データ、赤い三角が検証データ、緑の線が機械学習モデルの予測曲線。予測曲線がデータ点をほぼ完璧に通過している様子を示しています。

このことから、機械学習モデルが、第一原理計算で得られたデータをほぼ正しく予測できていることを確認できます。

## 発展：AIの「迷い」を可視化する（不確実性の評価）

多項式回帰では、データがない領域でも自信満々に線を引いて予測してしまいました。
しかし、研究の現場では「この予測はどれくらい信用できるか？」という**不確実性（Uncertainty）**の情報が極めて重要です。

ここでは、**ガウス過程回帰（Gaussian Process Regression）** という手法を使って、予測の「信頼区間（AIの自信のなさ）」を可視化してみましょう。

```python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

# ガウス過程回帰モデルの構築（カーネル関数の定義）
# RBFカーネル：滑らかな変化を表現
kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, alpha=0.01)

# 学習
gp.fit(X_train_scaled, y_train_scaled)

# 予測（標準偏差 sigma も一緒に出力させる）
# 広い範囲（25〜55 Å^3）で予測させてみる
wide_volume_range = np.linspace(25, 55, 100).reshape(-1, 1)
wide_volume_scaled = scaler_X.transform(wide_volume_range)

y_pred_scaled, sigma_scaled = gp.predict(wide_volume_scaled, return_std=True)

# スケールを元に戻す
y_pred_gp = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
sigma_gp = sigma_scaled * scaler_y.scale_[0] # 標準偏差のスケール変換

# グラフ化：信頼区間（95%）を帯として表示
plt.figure(figsize=(10, 6))
plt.plot(wide_volume_range, y_pred_gp, 'b-', label='Prediction')
plt.fill_between(wide_volume_range.ravel(),
                 y_pred_gp - 1.96 * sigma_gp,
                 y_pred_gp + 1.96 * sigma_gp,
                 alpha=0.2, color='blue', label='95% Confidence Interval')
plt.scatter(X_train, y_train, c='r', s=50, label='Training Data', zorder=3)

plt.xlabel('Volume ($\AA^3$)')
plt.ylabel('Energy (eV)')
plt.title('Uncertainty Quantification with Gaussian Process')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**結果の解釈:**
出力されたグラフを見ると、**赤い点（学習データ）がある付近では青い帯（不確実性）が非常に狭く**なっていますが、**データがない右端や左端（外挿領域）に行くほど、帯がラッパのように広がっている**のが分かります。

*   **外挿の危険性**: 帯が広がっている領域での予測は信用できません。
*   **Active Learning**: 次に計算すべきなのは、予測値が良い場所だけでなく、この「帯が広い（よく分かっていない）場所」です。
*   **ネガティブデータの価値**: もし「失敗データ」であっても、そこにデータ点があれば、その周辺の「帯」を狭めて、AIの理解を助けることができます。

これが、最新のデータ駆動型材料開発における「地図」の読み方です。

### 新しい体積値での予測

学習したモデルを使って、計算していない新しい体積値でのエネルギーを予測してみましょう。

```python
# 新しい体積値（例：38.5 Å³）での予測
new_volume = np.array([[38.5]])
new_volume_scaled = scaler_X.transform(new_volume)
predicted_energy_scaled = poly_reg.predict(new_volume_scaled)
predicted_energy = scaler_y.inverse_transform(predicted_energy_scaled.reshape(-1, 1))[0, 0]

print(f"入力体積: {new_volume[0, 0]:.3f} Å³")
print(f"予測エネルギー: {predicted_energy:.6f} eV")
```

**出力例：**

```term
入力体積: 38.500 Å³
予測エネルギー: -10.746575 eV
```

## EOSフィッティングと機械学習の違い

前章で行った「EOSフィッティング」と、本章で行った「機械学習（多項式回帰）」は、どちらもグラフに曲線を当てはめているように見えますが、その**目的とアプローチ**には明確な違いがあります。

| | EOS（状態方程式） | 機械学習（多項式回帰） |
| :--- | :--- | :--- |
| **アプローチ** | **物理モデル主導** (Theory-driven) | **データ主導** (Data-driven) |
| **数式の由来** | 物理学の理論から導かれた式<br>(Birch-Murnaghan式など) | データの形に合う数学的な関数<br>(多項式、ニューラルネットなど) |
| **主な目的** | **物性値の導出**<br>(体積弾性率、平衡定数など) | **未知データの予測**<br>(高速化、計算コスト削減) |
| **パラメータ** | 物理的な意味を持つ<br>($B_0$=硬さ、$V_0$=平衡体積) | 物理的な意味は持たない<br>(単なる重み係数) |

*   **EOS** は、「物理的に正しい式」を前提として、実験や計算データから物質の本質的な性質（硬さなど）を理解するために使います。
*   **機械学習** は、背後にある物理法則が複雑で不明な場合でも、データから傾向を学習し、新しいデータに対して高速に予測を行う（代理モデルを作る）ために使います。

## データ駆動型アプローチの意義

本章で実践したように、**機械学習モデルは、計算コストの高い第一原理計算の「代理モデル（Surrogate Model）」** として機能します。

- **高速予測**: 第一原理計算が数分〜数時間かかるのに対し、機械学習による予測は数ミリ秒で完了します。
- **計算コスト削減**: 必要な計算点数を減らし、中間点のエネルギーを機械学習で補間できます。
- **材料探索への拡張**: 複数の材料データを蓄積すれば、組成や構造から物性を直接予測するモデルを構築できます。

この「計算でデータを生成 → 機械学習で予測」のサイクルこそが、データ駆動型材料開発の核心です。

## 発展：自律的な材料探索へ（Advanced）

本講座では基礎的な予測モデルを構築しましたが、最先端の研究現場では、さらに高度な手法が活用されています。

### Delta Learning（差分学習）
計算データは量は稼げますが、実験値との間にはどうしても誤差（汎関数の近似など）が残ります。そこで、**「大量の計算データで大まかな傾向を学習」**させ、**「少数の実験データでその誤差（残差）を補正」**する **Delta Learning** という手法が有効です。これにより、計算の網羅性と実験の正確性を両立できます。

### Active Learning（能動学習）
AIがただ予測するだけでなく、「次にどのデータを計算（実験）すべきか」を人間に提案する手法です。
AIは、予測値が高い（有望な）物質だけでなく、**「予測の不確実性（Uncertainty）が高い物質」**、つまり「まだよく分かっていない領域」のデータを優先的に集めるよう提案します。これにより、効率的に知識を獲得し、決定境界の精度を高めることができます。

### 自動化と自律化（Autonomous Agent）
数千〜数万件の計算を行うハイスループット計算では、エラー（SCF不収束など）が頻発します。最新のシステムでは、**Custodian**（管理人）と呼ばれるエージェントプログラムが、エラーの内容を解析し、パラメータを自動調整して再計算を行うなど、人間の介入なしにデータを生成・管理する仕組み（**Machine Actionability**）が整えられつつあります。
