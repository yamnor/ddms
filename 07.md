---
title: 📘 実習⑥ | 自律的な材料探索へ
---

前章では、機械学習モデル（多項式回帰・ガウス過程回帰）を用いて物性を予測する方法を学びました。

本章では、さらに一歩進んで、**AIが「次にどのような実験・計算を行うべきか」を人間に提案する**、自律的な材料探索（Autonomous Materials Search）のプロセスを体験します。

## なぜ自律探索が必要なのか？

未知の材料空間は広大です。すべての組成・構造を計算したり実験したりすることは、時間的・コスト的に不可能です。

そこで、**「できるだけ少ない試行回数で、欲しい材料（または正しい物理法則）を見つけ出す」** ための戦略が必要になります。

これを実現するのが **アクティブラーニング（能動学習）** です。

## 不確実性に基づく探索（Uncertainty Sampling）

前章で紹介した **ガウス過程回帰（GPR）** を用いて、AIが「迷っている場所（不確実性が高い場所）」を特定し、そこを優先的に探索するシミュレーションを行ってみましょう。

### 1. 状況設定

- **目的**: シリコンのエネルギー曲線の全体像（特に谷底の位置）を、最小限の計算回数で明らかにしたい。
- **初期状態**: データは極端に少ない状態（例：2点のみ）からスタートします。
- **AIの役割**: 現在のデータからモデルを作成し、**「次にどこを計算すれば、最も情報が得られるか（不確実性が減るか）」**を提案します。

### 2. 探索ループの実装

以下のコードを実行して、AIがどのようにデータを「欲しがる」か観察してください。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.preprocessing import StandardScaler

# --- 準備：真の関数（答え）と初期データ ---
def true_function(x):
    # 簡易的なポテンシャル曲線のモデル (放物線に近い形)
    return 0.5 * (x - 40)**2 - 10.8

# 探索範囲 (Volume: 35 ~ 45)
X_pool = np.linspace(35, 45, 100).reshape(-1, 1)
y_true = true_function(X_pool).ravel()

# 初期データ: 両端の2点だけ知っている状態からスタート
X_train = np.array([[35.0], [45.0]])
y_train = true_function(X_train).ravel()

# スケーラーの準備
scaler_X = StandardScaler()
scaler_y = StandardScaler()

# --- 探索ループ (3回繰り返す) ---
for step in range(1, 4):
    print(f"--- Step {step} ---")
    
    # 1. データの正規化
    scaler_X.fit(X_train) # 毎回fitし直す（データが増えるため）
    X_train_scaled = scaler_X.transform(X_train)
    X_pool_scaled = scaler_X.transform(X_pool)
    
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
    
    # 2. ガウス過程回帰モデルの学習
    kernel = C(1.0) * RBF(1.0)
    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, alpha=0.001)
    gp.fit(X_train_scaled, y_train_scaled)
    
    # 3. 予測と不確実性(sigma)の算出
    y_pred_scaled, sigma_scaled = gp.predict(X_pool_scaled, return_std=True)
    
    # スケールを元に戻す
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
    sigma = sigma_scaled * scaler_y.scale_[0]
    
    # 4. 次の点の選定（獲得関数：最大不確実性）
    # sigma (不確実性) が最も大きい点を探す
    next_idx = np.argmax(sigma)
    next_x = X_pool[next_idx]
    max_uncertainty = sigma[next_idx]
    
    print(f"AIの提案: Volume = {next_x[0]:.2f} A^3 (不確実性: {max_uncertainty:.4f})")
    
    # --- 可視化 ---
    plt.figure(figsize=(8, 4))
    plt.plot(X_pool, y_true, 'k--', label='True Function', alpha=0.5) # 正解（カンニング）
    plt.plot(X_pool, y_pred, 'b-', label='AI Prediction')
    plt.fill_between(X_pool.ravel(), y_pred - 1.96*sigma, y_pred + 1.96*sigma, alpha=0.2, color='blue')
    plt.scatter(X_train, y_train, c='red', s=50, label='Known Data', zorder=5)
    plt.scatter(next_x, y_pred[next_idx], c='green', s=100, marker='*', label='Next Point', zorder=6)
    plt.title(f'Step {step}: N={len(X_train)}')
    plt.xlabel('Volume')
    plt.ylabel('Energy')
    plt.legend()
    plt.show()
    
    # 5. 「実験」を行い、データを追加する
    # ここでは関数値を計算してリストに追加
    new_y = true_function(next_x).ravel()
    X_train = np.vstack([X_train, next_x])
    y_train = np.concatenate([y_train, new_y])
```

### 結果の解説

1.  **Step 1**: 両端しかデータがないため、AIは「真ん中が一番わからない（帯が広い）」と判断し、真ん中（Volume=40付近）を提案します。
2.  **Step 2**: 真ん中のデータが得られたことで、全体の放物線の形状がおおよそ見えてきます。次にAIは、データの間隔が広い部分を提案します。
3.  **Step 3**: わずか数点のデータで、真の関数（点線）をほぼ完璧に再現できるようになります。

このように、**「何でもかんでも計算する」のではなく、「AIが迷っているところをピンポイントで計算する」** ことで、効率的な材料探索が可能になります。

## その他の発展的トピック

### Delta Learning（差分学習）

**「計算は速いがズレがある、実験は正しいが遅い」** というジレンマを解決する手法です。

**考え方**: 

$$ E_{\text{exp}}(V) \approx E_{\text{PBE}}(V) + \Delta(V) $$

実験値 $E_{\text{exp}}$ を直接予測するのではなく、計算値 $E_{\text{PBE}}$ との「差分 $\Delta(V)$」を機械学習モデルに学習させます。

**メリット**: 

差分 $\Delta(V)$ は、全エネルギーそのものよりも滑らかで単純な関数になることが多いため、少数の実験データでも高精度に学習できます。

### 自動化と自律化（Autonomous Agent）

ハイスループット計算において、人間が数千件の入力ファイルを手作りし、エラーが出るたびに修正するのは非効率です。

そこで、**Custodian**（管理人）や **Agent** と呼ばれるプログラムが導入されています。

1.  **Jobの投入**: 計算条件を決めてサーバーに投げる。
2.  **エラー監視**: 計算が止まったらログを解析する（「収束しなかった」「メモリ不足」など）。
3.  **自動修復**: エラーの種類に応じて、適切な対処（混合パラメータの変更、メモリ増設など）を行って再計算する。
4.  **データベース登録**: 成功したデータを自動でDBに格納する。

この仕組みにより、研究者は「どの物質を計算するか」という戦略決定に集中できるようになります。
